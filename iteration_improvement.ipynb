{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2023-10-13T18:09:39.034279200Z",
     "start_time": "2023-10-13T18:09:39.008243900Z"
    }
   },
   "outputs": [],
   "source": [
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "df_train = pd.read_csv('research_train_set.csv')\n",
    "num_dates = df_train[\"date_id\"].nunique()  # this number should be 401\n",
    "num_seconds_in_bucket = df_train[\"seconds_in_bucket\"].nunique()  # this number should be 55\n",
    "col2index_map = {key: value for (key, value) in zip(df_train.columns, range(len(df_train.columns)))}"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-13T18:09:51.767737100Z",
     "start_time": "2023-10-13T18:09:39.417835700Z"
    }
   },
   "id": "8ec33778f70d0f94"
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "def factor(current_data: np.ndarray, hist_list: list) -> np.ndarray:\n",
    "    '''\n",
    "    This will be the main function to design your factors for the competition. Please\n",
    "    define only one factor here each time. We provide you with:\n",
    "\n",
    "    current_data: The numpy array that contains the data up to the current date_id and\n",
    "    seconds_in_bucket in the loop\n",
    "\n",
    "    hist_list: A list for you to save the previous factor values (optional). For instance,\n",
    "    if you are calculating a 100-day Moving Averge (MA), then you can save the first calculated\n",
    "    MA in hist_list, and then for the next MA calculation, you can use the saved ones.\n",
    "    '''\n",
    "    ###################### ADD YOUR CODE HERE FOR FACTORS DESIGN ######################\n",
    "    res = current_data[:, [col2index_map['stock_id'], col2index_map['date_id'], col2index_map['seconds_in_bucket']]]\n",
    "    res = current_data[:, col2index_map['ask_price']] - current_data[:, col2index_map['bid_price']]\n",
    "    return res  # The return value MUSE BE a numpy array"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-13T18:09:51.782016200Z",
     "start_time": "2023-10-13T18:09:51.770499500Z"
    }
   },
   "id": "b82719e587cdd10e"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Original version"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "c891d7028ff6a0ad"
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "hist_list = []\n",
    "# tongfei's version\n",
    "all_test = np.empty((200*num_seconds_in_bucket*2000,16))\n",
    "final_factor_value = np.empty((200*num_seconds_in_bucket*2000,))\n",
    "current_row = 0\n",
    "for date_id in range(num_dates):\n",
    "    query_time = 0.\n",
    "    paste_time = 0.\n",
    "    factor_time = 0.\n",
    "    for seconds_in_bucket in range(num_seconds_in_bucket):\n",
    "        seconds_in_bucket *= 10\n",
    "        # part 1: query data\n",
    "        time_start_query = time.time()\n",
    "        new_test_data = np.array(df_train\n",
    "                                .query(f'date_id == {date_id} & seconds_in_bucket == {seconds_in_bucket}')\n",
    "                                .drop(columns = [\"target\"])\n",
    "                                .reset_index(drop=True))\n",
    "        time_end_query = time.time()\n",
    "        # part 2: paste data to all_test\n",
    "        time_paste_data_start = time.time()\n",
    "        all_test[current_row:current_row+len(new_test_data), :] = new_test_data\n",
    "        time_paste_data_end = time.time()\n",
    "        # part 3: calculate factor\n",
    "        \n",
    "        time_factor_calc_start = time.time()\n",
    "        final_factor_value[current_row:current_row+len(new_test_data),] = factor(current_data=all_test[current_row:current_row+len(new_test_data),:], \n",
    "                                                                                 hist_list=hist_list)\n",
    "        time_factor_calc_end = time.time()\n",
    "        \n",
    "        query_time += time_end_query - time_start_query\n",
    "        paste_time += time_paste_data_end - time_paste_data_start\n",
    "        factor_time += time_factor_calc_end - time_factor_calc_start\n",
    "    break"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-13T18:09:53.672549300Z",
     "start_time": "2023-10-13T18:09:51.779022100Z"
    }
   },
   "id": "bd2d1bad843ab528"
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "query time: 1.8800549507141113, 99.47396452376381%\n",
      "paste time: 0.008942604064941406, 0.4731544039008873%\n",
      "factor time: 0.00099945068359375, 0.05288107233530233%\n",
      "predicted_process_time: 12.599980036417643 minutes\n"
     ]
    }
   ],
   "source": [
    "# time expense for 1 day\n",
    "print(f'query time: {query_time}, {query_time/(query_time+paste_time+factor_time)*100}%')\n",
    "print(f'paste time: {paste_time}, {paste_time/(query_time+paste_time+factor_time)*100}%')\n",
    "print(f'factor time: {factor_time}, {factor_time/(query_time+paste_time+factor_time)*100}%')\n",
    "print(f'predicted_process_time: {(query_time+paste_time+factor_time)*400/60} minutes')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-13T18:09:53.682488700Z",
     "start_time": "2023-10-13T18:09:53.677489300Z"
    }
   },
   "id": "69fd311b9296a2fd"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# It seems like the query is the problem. query tool is sloooow. using native pandas slicer"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "d36c53a1b0bdf654"
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     stock_id  date_id  seconds_in_bucket  imbalance_size  \\\n",
      "0           0        0                540       906957.07   \n",
      "1           1        0                540        92236.28   \n",
      "2           2        0                540            0.00   \n",
      "3           3        0                540      2446727.30   \n",
      "4           4        0                540            0.00   \n",
      "..        ...      ...                ...             ...   \n",
      "186       194        0                540       583278.56   \n",
      "187       195        0                540       304130.06   \n",
      "188       196        0                540      1595817.99   \n",
      "189       197        0                540       354616.47   \n",
      "190       198        0                540      4357623.66   \n",
      "\n",
      "     imbalance_buy_sell_flag  reference_price  matched_size  far_price  \\\n",
      "0                          1         0.999383   27183793.68   1.000241   \n",
      "1                         -1         1.000425    3439290.16   0.999955   \n",
      "2                          0         0.999877    5121992.88   0.999877   \n",
      "3                          1         0.999440   42482476.86   0.999956   \n",
      "4                          0         1.001191   41712582.55   1.001191   \n",
      "..                       ...              ...           ...        ...   \n",
      "186                        1         1.000809    5213804.47   1.004044   \n",
      "187                       -1         0.998402   33493937.72   0.998176   \n",
      "188                       -1         0.999695    9965822.60   0.997147   \n",
      "189                       -1         0.999977    9118830.62   0.999836   \n",
      "190                        1         0.999433   52151663.96   1.000117   \n",
      "\n",
      "     near_price  bid_price   bid_size  ask_price   ask_size       wap  \\\n",
      "0      0.999919   0.999276  111912.00   0.999383    9793.35  0.999375   \n",
      "1      1.000425   1.000308   19235.99   1.001248   45494.13  1.000587   \n",
      "2      0.999877   0.999772   18985.00   1.000667     380.04  1.000649   \n",
      "3      0.999870   0.999354   46468.00   0.999440   10456.20  0.999424   \n",
      "4      1.001191   1.000914   70821.87   1.001260    4781.04  1.001238   \n",
      "..          ...        ...        ...        ...        ...       ...   \n",
      "186    1.001839   1.000368  100820.46   1.000809   12318.86  1.000761   \n",
      "187    0.998176   0.998402  101303.04   0.998515   97163.00  0.998460   \n",
      "188    0.999695   0.999695   45289.30   0.999844  389186.14  0.999710   \n",
      "189    0.999836   0.999907  203793.36   1.000260   85062.00  1.000156   \n",
      "190    0.999946   0.999262  715756.14   0.999433  360743.50  0.999376   \n",
      "\n",
      "     time_id     row_id  \n",
      "0         54    0_540_0  \n",
      "1         54    0_540_1  \n",
      "2         54    0_540_2  \n",
      "3         54    0_540_3  \n",
      "4         54    0_540_4  \n",
      "..       ...        ...  \n",
      "186       54  0_540_194  \n",
      "187       54  0_540_195  \n",
      "188       54  0_540_196  \n",
      "189       54  0_540_197  \n",
      "190       54  0_540_198  \n",
      "\n",
      "[191 rows x 16 columns]\n"
     ]
    }
   ],
   "source": [
    "hist_list = []\n",
    "# kevin's readability version\n",
    "all_test = np.empty((200*num_seconds_in_bucket*2000,16))\n",
    "final_factor_value = np.empty((200*num_seconds_in_bucket*2000,))\n",
    "current_row = 0\n",
    "for date_id in range(num_dates):\n",
    "    query_time = 0.\n",
    "    paste_time = 0.\n",
    "    factor_time = 0.\n",
    "    for seconds_in_bucket in range(num_seconds_in_bucket):\n",
    "        seconds_in_bucket *= 10\n",
    "        # part 1: query data\n",
    "        time_start_query = time.time()\n",
    "        new_test_data = df_train[(df_train['date_id'] == date_id) & (df_train['seconds_in_bucket'] == seconds_in_bucket)].drop(columns = [\"target\"]).reset_index(drop=True)\n",
    "        time_end_query = time.time()\n",
    "        # part 2: paste data to all_test\n",
    "        time_paste_data_start = time.time()\n",
    "        all_test[current_row:current_row+len(new_test_data), :] = new_test_data\n",
    "        time_paste_data_end = time.time()\n",
    "        # part 3: calculate factor\n",
    "        \n",
    "        time_factor_calc_start = time.time()\n",
    "        final_factor_value[current_row:current_row+len(new_test_data),] = factor(current_data=all_test[current_row:current_row+len(new_test_data),:], \n",
    "                                                                                 hist_list=hist_list)\n",
    "        time_factor_calc_end = time.time()\n",
    "        \n",
    "        query_time += time_end_query - time_start_query\n",
    "        paste_time += time_paste_data_end - time_paste_data_start\n",
    "        factor_time += time_factor_calc_end - time_factor_calc_start\n",
    "    print(new_test_data)\n",
    "    break"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-13T18:10:53.628966Z",
     "start_time": "2023-10-13T18:10:52.724940100Z"
    }
   },
   "id": "d3ba12c9eb3c06c6"
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "query time: 0.8770031929016113, 98.42784865133315%\n",
      "paste time: 0.011001348495483398, 1.234703674329145%\n",
      "factor time: 0.0030066967010498047, 0.3374476743377078%\n",
      "predicted_process_time: 5.940074920654297 minutes\n"
     ]
    }
   ],
   "source": [
    "# time expense for 1 day\n",
    "print(f'query time: {query_time}, {query_time/(query_time+paste_time+factor_time)*100}%')\n",
    "print(f'paste time: {paste_time}, {paste_time/(query_time+paste_time+factor_time)*100}%')\n",
    "print(f'factor time: {factor_time}, {factor_time/(query_time+paste_time+factor_time)*100}%')\n",
    "print(f'predicted_process_time: {(query_time+paste_time+factor_time)*400/60} minutes')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-13T18:11:02.379246300Z",
     "start_time": "2023-10-13T18:11:02.368056100Z"
    }
   },
   "id": "34be4a49b696324d"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# We can also be one step more efficient by removing the overhead altogether\n",
    "idea from https://stackoverflow.com/questions/57208997/looking-for-the-fastest-way-to-slice-a-row-in-a-huge-pandas-dataframe \n",
    "and \n",
    "https://stackoverflow.com/questions/49222788/converting-pandas-dataframe-to-dictionary-using-index-option-with-a-non-unique"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "652ea4b304a46b09"
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [],
   "source": [
    "overhead_start = time.time()\n",
    "df_train['slice_index'] = df_train['date_id'].astype(str) + '_' + df_train['seconds_in_bucket'].astype(str)  # 3 seconds\n",
    "dic_sorted = df_train.drop(columns=['target']).groupby('slice_index').agg(lambda x: x.tolist()).to_dict('index')  # 1min\n",
    "overhead_end = time.time()\n",
    "overhead_time = overhead_end - overhead_start"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-13T18:33:06.799648Z",
     "start_time": "2023-10-13T18:32:46.019233200Z"
    }
   },
   "id": "37abc469acff173"
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     stock_id  date_id  seconds_in_bucket  imbalance_size  \\\n",
      "0           0        0                540       906957.07   \n",
      "1           1        0                540        92236.28   \n",
      "2           2        0                540            0.00   \n",
      "3           3        0                540      2446727.30   \n",
      "4           4        0                540            0.00   \n",
      "..        ...      ...                ...             ...   \n",
      "186       194        0                540       583278.56   \n",
      "187       195        0                540       304130.06   \n",
      "188       196        0                540      1595817.99   \n",
      "189       197        0                540       354616.47   \n",
      "190       198        0                540      4357623.66   \n",
      "\n",
      "     imbalance_buy_sell_flag  reference_price  matched_size  far_price  \\\n",
      "0                          1         0.999383   27183793.68   1.000241   \n",
      "1                         -1         1.000425    3439290.16   0.999955   \n",
      "2                          0         0.999877    5121992.88   0.999877   \n",
      "3                          1         0.999440   42482476.86   0.999956   \n",
      "4                          0         1.001191   41712582.55   1.001191   \n",
      "..                       ...              ...           ...        ...   \n",
      "186                        1         1.000809    5213804.47   1.004044   \n",
      "187                       -1         0.998402   33493937.72   0.998176   \n",
      "188                       -1         0.999695    9965822.60   0.997147   \n",
      "189                       -1         0.999977    9118830.62   0.999836   \n",
      "190                        1         0.999433   52151663.96   1.000117   \n",
      "\n",
      "     near_price  bid_price   bid_size  ask_price   ask_size       wap  \\\n",
      "0      0.999919   0.999276  111912.00   0.999383    9793.35  0.999375   \n",
      "1      1.000425   1.000308   19235.99   1.001248   45494.13  1.000587   \n",
      "2      0.999877   0.999772   18985.00   1.000667     380.04  1.000649   \n",
      "3      0.999870   0.999354   46468.00   0.999440   10456.20  0.999424   \n",
      "4      1.001191   1.000914   70821.87   1.001260    4781.04  1.001238   \n",
      "..          ...        ...        ...        ...        ...       ...   \n",
      "186    1.001839   1.000368  100820.46   1.000809   12318.86  1.000761   \n",
      "187    0.998176   0.998402  101303.04   0.998515   97163.00  0.998460   \n",
      "188    0.999695   0.999695   45289.30   0.999844  389186.14  0.999710   \n",
      "189    0.999836   0.999907  203793.36   1.000260   85062.00  1.000156   \n",
      "190    0.999946   0.999262  715756.14   0.999433  360743.50  0.999376   \n",
      "\n",
      "     time_id     row_id  \n",
      "0         54    0_540_0  \n",
      "1         54    0_540_1  \n",
      "2         54    0_540_2  \n",
      "3         54    0_540_3  \n",
      "4         54    0_540_4  \n",
      "..       ...        ...  \n",
      "186       54  0_540_194  \n",
      "187       54  0_540_195  \n",
      "188       54  0_540_196  \n",
      "189       54  0_540_197  \n",
      "190       54  0_540_198  \n",
      "\n",
      "[191 rows x 16 columns]\n"
     ]
    }
   ],
   "source": [
    "hist_list = []\n",
    "# kevin's readability version\n",
    "all_test = np.empty((200*num_seconds_in_bucket*2000,16))\n",
    "final_factor_value = np.empty((200*num_seconds_in_bucket*2000,))\n",
    "current_row = 0\n",
    "for date_id in range(num_dates):\n",
    "    query_time = 0.\n",
    "    paste_time = 0.\n",
    "    factor_time = 0.\n",
    "    for seconds_in_bucket in range(num_seconds_in_bucket):\n",
    "        seconds_in_bucket *= 10\n",
    "        # part 1: query data\n",
    "        time_start_query = time.time()\n",
    "        new_test_data = pd.DataFrame(dic_sorted[f'{date_id}_{seconds_in_bucket}'])\n",
    "        time_end_query = time.time()\n",
    "        # part 2: paste data to all_test\n",
    "        time_paste_data_start = time.time()\n",
    "        all_test[current_row:current_row+len(new_test_data), :] = new_test_data\n",
    "        time_paste_data_end = time.time()\n",
    "        # part 3: calculate factor\n",
    "        \n",
    "        time_factor_calc_start = time.time()\n",
    "        final_factor_value[current_row:current_row+len(new_test_data),] = factor(current_data=all_test[current_row:current_row+len(new_test_data),:], \n",
    "                                                                                 hist_list=hist_list)\n",
    "        time_factor_calc_end = time.time()\n",
    "        \n",
    "        query_time += time_end_query - time_start_query\n",
    "        paste_time += time_paste_data_end - time_paste_data_start\n",
    "        factor_time += time_factor_calc_end - time_factor_calc_start\n",
    "    print(new_test_data)\n",
    "    break"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-13T18:33:06.913912800Z",
     "start_time": "2023-10-13T18:33:06.809650100Z"
    }
   },
   "id": "e8ee44d2706311bf"
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "overhead time: 22.133418083190918 seconds\n",
      "query time: 0.05000185966491699, 81.9717175823145%\n",
      "paste time: 0.008998394012451172, 14.751727588255525%\n",
      "factor time: 0.0019986629486083984, 3.2765548294299744%\n",
      "predicted_process_time: 0.40665944417317706 minutes\n"
     ]
    }
   ],
   "source": [
    "# time expense for 1 day\n",
    "print(f'overhead time: {overhead_time} seconds')\n",
    "print(f'query time: {query_time}, {query_time/(query_time+paste_time+factor_time)*100}%')\n",
    "print(f'paste time: {paste_time}, {paste_time/(query_time+paste_time+factor_time)*100}%')\n",
    "print(f'factor time: {factor_time}, {factor_time/(query_time+paste_time+factor_time)*100}%')\n",
    "print(f'predicted_process_time: {(query_time+paste_time+factor_time)*400/60} minutes')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-13T18:42:54.185871500Z",
     "start_time": "2023-10-13T18:42:54.180096100Z"
    }
   },
   "id": "6305a7f0745c0276"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
